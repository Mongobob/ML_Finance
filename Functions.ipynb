{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (0.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: import_ipynb in /Users/Simon/opt/anaconda3/lib/python3.7/site-packages (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.imputation.mice import MICE, MICEData\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.constraints import maxnorm\n",
    "from matplotlib import pyplot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "# import fancyimpute\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#import all the functions we wrote ourselves\n",
    "%pip install import_ipynb\n",
    "import import_ipynb\n",
    "#import Functions as functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_keyfigures_and_Companies(arr1,arr3):\n",
    "    '''\n",
    "    This Dataframe joins keyfigures and companylist using Permno as key\n",
    "    :param arr1: Pandas-Dataframe of keyfigures\n",
    "    :param arr3: Pandas-Dataframe of S&PCompanylist\n",
    "    :return: joint Dataframe on Permno\n",
    "    '''\n",
    "    assert type(arr1)== pd.DataFrame\n",
    "    assert type(arr3) == pd.DataFrame\n",
    "    arr1=pd.DataFrame(arr1)\n",
    "    arr3 = pd.DataFrame(arr3)\n",
    "    arr3=arr3.rename(columns={'PERMNO':'permno'})\n",
    "    output=pd.merge_asof(arr1,arr3,'permno')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_key(arr):\n",
    "    '''\n",
    "    This function adds a unique-key to a dataframe so it can be used as identifier of a row. Key = public_date + Ticker\n",
    "    :param arr: pandas dataframe\n",
    "    :return: df + col key out of public-date and TICKER\n",
    "    '''\n",
    "    assert type(arr)==pd.DataFrame\n",
    "    arr=pd.DataFrame(arr)\n",
    "    arr['key'] = arr.public_date.astype(str) + arr.TICKER.astype(str)\n",
    "    arr['key']=arr['key'].astype(str)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_Ratings_and_Rest(rest,arr2):\n",
    "    '''\n",
    "    This function combines the 2 already combined Dataframes with the Ratings using date and ticker\n",
    "    :param rest: Pandas-Dataframe of keyfigures and S&PCompanylist\n",
    "    :param arr2: Pandas-Dataframe of Ratings\n",
    "    :return: joint Pandas-Dataframe of all. Uncleaned\n",
    "    '''\n",
    "    assert type(rest)== pd.DataFrame\n",
    "    assert type(arr2) == pd.DataFrame\n",
    "    rest=pd.DataFrame(rest)\n",
    "    arr2 = pd.DataFrame(arr2)\n",
    "    arr2 = arr2.rename(columns={'tic': 'TICKER'})\n",
    "    arr2['public_date']=arr2['datadate']\n",
    "    #Since the format of the date does not match between the dataframe, they are going to be reformatted first\n",
    "    rest['public_date']=rest['public_date'].astype(str)\n",
    "    rest['public_date'] = rest['public_date'].str.replace(r'/', '')\n",
    "    arr2['public_date'] = arr2['public_date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "    rest['public_date'] = rest['public_date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "    \n",
    "    #Now that the formats are matching, the function add_key(DATAFRAME) is used, to help merge the 2 Dataframes together\n",
    "    arr2=add_key(arr2)\n",
    "    rest=add_key(rest)\n",
    "    output = rest.merge(arr2, how='left', on=['key'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagging_Ratings_1M(path):\n",
    "    '''\n",
    "    This function creates a nested list of keys and the according rating of the previous month for merging with pandas-df-\n",
    "    :param path: path to csv-file with data\n",
    "    :return: return list with tuple (key, Rating), so it can be merged to Dataframe. Empty values represented by None.\n",
    "    '''\n",
    "    output=[['key', 'Lagging-Rating_1M']]\n",
    "    with open(path,'r') as f:\n",
    "        #create comparision-list:\n",
    "        compdict={}\n",
    "        for line in f.readlines():\n",
    "            li = line.split(',')\n",
    "            Kürzel=str(li[79][:7])+str(li[79][10:])\n",
    "\n",
    "            compdict[Kürzel]=li[81]\n",
    "        f.close()\n",
    "        with open(path,'r') as r:\n",
    "            for line in r.readlines()[1:]:\n",
    "                key=None\n",
    "                lagrating=None\n",
    "                sublis=()\n",
    "                newdate=None\n",
    "                liste=line.split(',')\n",
    "                #create date of previous month of this line\n",
    "                newdate=str(liste[4][:5])+str(liste[4][5:7])\n",
    "\n",
    "                #if month between 01 and 08\n",
    "                if int(liste[4][5:7]) in range(9):\n",
    "                    newdate = str(liste[4][:5]) + '0'+str(int(liste[4][5:7]) + 1)\n",
    "\n",
    "                #if month >=9\n",
    "                elif 11>=int(liste[4][5:7])>=9:\n",
    "                    newdate =str(liste[4][:5])+str(int(liste[4][5:7])+1)\n",
    "\n",
    "                #if month ==12 (We gotta subtract a Year too)\n",
    "                elif int(liste[4][5:7])==12:\n",
    "                    newdate = str(int(liste[4][:4])+1) + '-01'\n",
    "                key = str(liste[4]) + str(liste[74])\n",
    "                newkey=newdate+str(liste[74])\n",
    "                try:\n",
    "                    #If the previous month exists with a rating, use this\n",
    "                    lagrating = compdict[newdate + str(liste[74])]\n",
    "                except KeyError:\n",
    "                    #if there is no previous-month-rating, fill-in None\n",
    "                    lagrating=None\n",
    "                sublis=[key,lagrating]\n",
    "                output.append(sublis)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_iterative_imputer(df):\n",
    "    \"\"\" \n",
    "    Impute the missing values (NaN) with the IterativeImputer\n",
    "    :param df: feature matrix with NaN values to be imputed\n",
    "    :return: imputed Pandas-Dataframe\n",
    "    \"\"\"\n",
    "    from sklearn.experimental import enable_iterative_imputer \n",
    "    from sklearn.impute import IterativeImputer\n",
    "    #Define all column with numeric values (the features)\n",
    "    num_cols = ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', \n",
    "                'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "                'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "                'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act',\n",
    "                'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct',\n",
    "                'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov',\n",
    "                'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "                'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'accrual', 'ptb',\n",
    "                'DIVYIELD', 'PEG_1yrforward', 'PEG_ltgforward']\n",
    "\n",
    "    # Copy df to df_imputed\n",
    "    df_imputed = df[num_cols].copy(deep=True)\n",
    "\n",
    "    # Initialize IterativeImputer\n",
    "    mice_imputer = IterativeImputer(max_iter=20)\n",
    "\n",
    "    # Impute using fit_tranform on df\n",
    "    df_imputed.iloc[:, :] = mice_imputer.fit_transform(df[num_cols])\n",
    "    \n",
    "    return df_imputed.iloc[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def feature_selection(x, y, thres):\n",
    "    \"\"\" \n",
    "    Find out, which the most important features are. Return a list of the most important features\n",
    "    which wil be used for the algorithms.\n",
    "    :param x: feature input without NaN values\n",
    "    :param y: classification input\n",
    "    :param thres: input as percentage value, features with relative importance over this value will be in the output \n",
    "    :return: the list of important features\n",
    "    \"\"\"\n",
    "    \n",
    "    feat_labels = x.columns[:]\n",
    "    \n",
    "    # Create Random Forest object, fit data and\n",
    "    # extract feature importance attributes\n",
    "    forest = RandomForestClassifier(random_state=1, class_weight='balanced')\n",
    "    forest.fit(x, y)\n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    #Define n as number of importances over the value thres\n",
    "    n = sum(importances > thres)\n",
    "    \n",
    "    # Get cumsum of the n most important features\n",
    "    feat_imp = np.sort(importances)[::-1]\n",
    "    sum_feat_imp = np.cumsum(feat_imp)[:n]\n",
    "    \n",
    "    # Sort output (by relative importance) and \n",
    "    # print top n features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for i in range(n):\n",
    "        print('{0:2d}) {1:7s} {2:6.4f}'.format(i + 1, \n",
    "                                           feat_labels[indices[i]],\n",
    "                                           importances[indices[i]]))\n",
    "        \n",
    "    \n",
    "    # Plot Feature Importance (both cumul., individual)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(n), importances[indices[:n]], align='center')\n",
    "    plt.xticks(range(n), feat_labels[indices[:n]], rotation=90)\n",
    "    plt.xlim([-1, n])\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Rel. Feature Importance')\n",
    "    plt.step(range(n), sum_feat_imp, where='mid', \n",
    "         label='Cumulative importance')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    \n",
    "    # Create a list with the important features for ML algorhithms\n",
    "    feature_list = [None] * n\n",
    "    for i in range(n):\n",
    "        feature_list[i] = feat_labels[indices[i]]\n",
    "    \n",
    "    # return the list of important features\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(X_train, Y_train,param_grid):\n",
    "    '''\n",
    "    This function performs a Logistic Regression on the X_train and Y_train and uses grid-cross validation on the datasets.\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "    lor = LogisticRegression(max_iter=100, tol=0.001,random_state=1, n_jobs=-1,solver='saga',warm_start=True) #increasing iterations to 1000 increases score by only 1% -> it is not worth the additional time\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('logreg', lor)])\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train,Y_train)\n",
    "    \n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_poly(X_train,Y_train,param_grid):\n",
    "    '''\n",
    "    This function uses Support Vector Machines on X_train and Y_train with a Polynomial Kernel Function.\n",
    "    It uses Grid-Crossvalidation to find the best hyperparameters for the dataset\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "    \n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_poly', SVC(kernel='poly', random_state=0, max_iter=100000))])\n",
    "\n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf(X_train,Y_train,param_grid):\n",
    "    '''\n",
    "    This function uses Support Vector Machines on X_train and Y_train with a Radial Basis Kernel Function(rbf).\n",
    "    It uses Grid-Crossvalidation to find the best hyperparameters for the dataset\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000))])\n",
    "    \n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf_bal(X_train,Y_train,param_grid):\n",
    "    '''\n",
    "    This function uses Support Vector Machines on X_train and Y_train with a Radial Basis Kernel Function(rbf).\n",
    "    It uses Grid-Crossvalidation to find the best hyperparameters for the dataset where we use balanced class weights.\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000, class_weight='balanced'))])\n",
    "\n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, Y_train, n_estimators, maxDepth, minSamplesNode, minSamplesLeaf):\n",
    "    \"\"\" \n",
    "    This function applies Random Forest Classifier on X_train and Y_train and uses grid-cross validation on the datasets.\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param n_estimators: array of values which will be tested for variable n_estimators\n",
    "    :param maxDepth: array of values which will be tested for variable max_depth\n",
    "    :param minSamplesNode: array of values which will be tested for variable min_samples_split\n",
    "    :param minSamplesLeaf: array of values which will be tested for variable min_samples_leaf\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter values to be tested\n",
    "    param_grid = {\"n_estimators\": n_estimators,\n",
    "                  'max_depth': maxDepth,\n",
    "                  'min_samples_split': minSamplesNode,\n",
    "                  'min_samples_leaf': minSamplesLeaf},\n",
    "\n",
    "    # Run brute-force grid search\n",
    "    grid = GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv= 5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(X_train, Y_train, hidden_layer, maxIter):\n",
    "    \"\"\" \n",
    "    This function applies MLP Classifier on X_train and Y_train and uses grid-cross validation on the datasets.\n",
    "    :param X_train: Training Set of X values\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param hidden_layer: array of values which will be tested for variable hidden_layer_sizes\n",
    "    :param maxIter: array of values which will be tested for variable max_iter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    mlp = MLPClassifier(random_state=0, solver= \"lbfgs\", warm_start= True)\n",
    "    \n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('neural', MLPClassifier())])\n",
    "    \n",
    "    # Define the hyperparameter values to be tested\n",
    "    param_grid = {\"neural__hidden_layer_sizes\" : hidden_layer,\n",
    "                  'neural__max_iter': maxIter},\n",
    "\n",
    "\n",
    "    # Run brute-force grid search\n",
    "    #solver \"lbfgs\" has proven to be the best\n",
    "    grid = GridSearchCV(pipe,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv= 5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model1(optimizer='Nadam',neurons=350,activ='sigmoid',dropout_rate=0.0):\n",
    "    '''\n",
    "    This function is a neural network design with one hidden layer\n",
    "    :param optimizer: Which optimizer to use\n",
    "    :param neurons: Amount of neurons in layer    \n",
    "    :param activ: Activation function\n",
    "    :param dropout_rate: dropout rate as percentage\n",
    "    :return: single layered neural network\n",
    "    '''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=36, activation=activ))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(17, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) #This loss function doesn't need OneHotEncoded Y-Variables.\n",
    "    return(model)\n",
    "\n",
    "def baseline_model2(optimizer='Nadam',neurons1=400,neurons2=150,activ1 = 'sigmoid', activ2 = 'sigmoid', dropout_rate1=0.0,dropout_rate2=0.0):\n",
    "    '''\n",
    "    This function is a neural network design with two hidden layers\n",
    "    :param optimizer: Which optimizer to use\n",
    "    :param neurons1: Amount of neurons in first hidden layer   \n",
    "    :param neurons2: Amount of neurons in second hidden layer   \n",
    "    :param activ1: Activation function in first hidden layer\n",
    "    :param activ2: Activation function in second hidden layer\n",
    "    :param dropout_rate1: dropout rate as percentage in first hidden layer\n",
    "    :param dropout_rate2: dropout rate as percentage in second hidden layer\n",
    "    :return: two layered neural network\n",
    "    '''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=36, activation=activ1))\n",
    "    model.add(Dropout(dropout_rate1))\n",
    "    model.add(Dense(neurons2, activation=activ2))\n",
    "    model.add(Dropout(dropout_rate2))\n",
    "    model.add(Dense(17, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) #This loss function doesn't need OneHotEncoded Y-Variables.\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def NeuralNet1layer(X_train_std,Y_train, param_grid):\n",
    "    '''\n",
    "    This function trains the neural network with the single hidden layer\n",
    "    :param X_train_std: Training Set of X values(standardized)\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "    model = KerasClassifier(build_fn=baseline_model1, epochs=100, batch_size=20,verbose=0)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3,n_jobs=-1) #We don't use a pipeline because of the limits of Keras sklearn wrapper which can't do a gridsearch over the hyperparameters when in a pipeline, also for some hyperparameters parallelizing is not possible due to the wrapper\n",
    "    grid_result = grid.fit(X_train_std, Y_train)\n",
    "    return(grid_result)\n",
    "\n",
    "def NeuralNet2layer(X_train_std,Y_train, param_grid):\n",
    "    '''\n",
    "    This function trains the neural network with the single hidden layer\n",
    "    :param X_train_std: Training Set of X values(standardized)\n",
    "    :param Y_train: Training Set of Y values(factorized)\n",
    "    :param param_grid: Grid of parameters to optimize over\n",
    "    :return: Cross-Validated optimal hyperparamer model fit\n",
    "    '''\n",
    "    model = KerasClassifier(build_fn=baseline_model2, epochs=100, batch_size=20,verbose=0)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1) #We don't use a pipeline because of the limits of Keras sklearn wrapper which can't do a gridsearch over the hyperparameters when in a pipeline\n",
    "    grid_result = grid.fit(X_train_std, Y_train)\n",
    "    return(grid_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
