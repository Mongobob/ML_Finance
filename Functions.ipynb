{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_iterative_imputer(df):\n",
    "    \"\"\" Impute the missing values (NaN) with the IterativeImputer\n",
    "    \n",
    "    Args:\n",
    "        df: feature matrix with NaN values to be imputed\n",
    "    \"\"\"\n",
    "    #Define all column with numeric values (the features)\n",
    "    num_cols = ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', \n",
    "                'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "                'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "                'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act',\n",
    "                'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct',\n",
    "                'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov',\n",
    "                'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "                'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'accrual', 'ptb',\n",
    "                'DIVYIELD', 'PEG_1yrforward', 'PEG_ltgforward']\n",
    "\n",
    "    # Copy df to df_imputed\n",
    "    df_imputed = df[num_cols].copy(deep=True)\n",
    "\n",
    "    # Initialize IterativeImputer\n",
    "    mice_imputer = IterativeImputer()\n",
    "\n",
    "    # Impute using fit_tranform on df\n",
    "    df_imputed.iloc[:, :] = mice_imputer.fit_transform(df[num_cols])\n",
    "    \n",
    "    return df_imputed.iloc[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def feature_selection(x, y, thres):\n",
    "    \"\"\" Find out, which the most important features are. Return a list of the most important features\n",
    "        which wil be used for the algorithms.\n",
    "    \n",
    "    Args:\n",
    "        x: feature input without NaN values\n",
    "        y: classification input\n",
    "        thres: input as percentage value, features with relative importance over this value will be in the output \n",
    "    \"\"\"\n",
    "    \n",
    "    feat_labels = x.columns[:]\n",
    "    \n",
    "    # Create Random Forest object, fit data and\n",
    "    # extract feature importance attributes\n",
    "    forest = RandomForestClassifier(random_state=1, class_weight='balanced')\n",
    "    forest.fit(x, y)\n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    #Define n as number of importances over the value thres\n",
    "    n = sum(importances > thres)\n",
    "    \n",
    "    # Get cumsum of the n most important features\n",
    "    feat_imp = np.sort(importances)[::-1]\n",
    "    sum_feat_imp = np.cumsum(feat_imp)[:n]\n",
    "    \n",
    "    # Sort output (by relative importance) and \n",
    "    # print top n features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for i in range(n):\n",
    "        print('{0:2d}) {1:7s} {2:6.4f}'.format(i + 1, \n",
    "                                           feat_labels[indices[i]],\n",
    "                                           importances[indices[i]]))\n",
    "        \n",
    "    \n",
    "    # Plot Feature Importance (both cumul., individual)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(n), importances[indices[:n]], align='center')\n",
    "    plt.xticks(range(n), feat_labels[indices[:n]], rotation=90)\n",
    "    plt.xlim([-1, n])\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Rel. Feature Importance')\n",
    "    plt.step(range(n), sum_feat_imp, where='mid', \n",
    "         label='Cumulative importance')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    \n",
    "    # Create a list with the important features for ML algorhithms\n",
    "    feature_list = [None] * n\n",
    "    for i in range(n):\n",
    "        feature_list[i] = feat_labels[indices[i]]\n",
    "    \n",
    "    # return the list of important features\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(X_train, Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)\n",
    "    '''\n",
    "    lor = LogisticRegression(max_iter=100, tol=0.001,random_state=1, n_jobs=-1,solver='saga',warm_start=True) #increasing iterations to 1000 increases score by only 1% -> it is not worth the additional time\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('logreg', lor)])\n",
    "\n",
    "    param_grid = {'logreg__penalty': ['elasticnet'], #elastic nets combines l1&l2\n",
    "                  'logreg__C':[6,6.5,7,7.5,8],\n",
    "                  'logreg__l1_ratio':[0,0.05,0.1,0.15,0.2,1]} #if 0, or 1 then l2 or l1 would be best. If between then the combination of both\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train,Y_train)\n",
    "    \n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_poly(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)\n",
    "    '''\n",
    "    \n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_poly', SVC(kernel='poly', random_state=0, max_iter=100000))])\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_poly__C': [900,1000,1100], \n",
    "                  'svm_poly__degree': [3,4,5],\n",
    "                  'svm_poly__gamma': [0,0.05,0.1],\n",
    "                  'svm_poly__coef0':[0.6]}  #Larger gridsearch yielded 0.6 to be the best coef0 with this combination. As it does not greatly change the cv accuracy(<1%) we don't include it in this grid search to lower the computing time.\n",
    "\n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)    \n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000))])\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_rbf__C': [100,150,200], \n",
    "                  'svm_rbf__gamma': [0.25,0.3,0.35]} \n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf_bal(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)    \n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000, class_weight='balanced'))])\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_rbf__C': [100,200,300], \n",
    "                  'svm_rbf__gamma': [0.25,0.3,0.35]} \n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
