{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def feature_selection(x, y, thres):\n",
    "    \"\"\" Find out, which the most important features are. Return a list of the most important features\n",
    "        which wil be used for the algorithms.\n",
    "    \n",
    "    Args:\n",
    "        x: feature input without NaN values\n",
    "        y: classification input\n",
    "        thres: input as percentage value, features with relative importance over this value will be in the output \n",
    "    \"\"\"\n",
    "    \n",
    "    feat_labels = x.columns[:]\n",
    "    \n",
    "    # Create Random Forest object, fit data and\n",
    "    # extract feature importance attributes\n",
    "    forest = RandomForestClassifier(random_state=1, class_weight='balanced')\n",
    "    forest.fit(x, y)\n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    #Define n as number of importances over the value thres\n",
    "    n = sum(importances > thres)\n",
    "    \n",
    "    # Get cumsum of the n most important features\n",
    "    feat_imp = np.sort(importances)[::-1]\n",
    "    sum_feat_imp = np.cumsum(feat_imp)[:n]\n",
    "    \n",
    "    # Sort output (by relative importance) and \n",
    "    # print top n features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for i in range(n):\n",
    "        print('{0:2d}) {1:7s} {2:6.4f}'.format(i + 1, \n",
    "                                           feat_labels[indices[i]],\n",
    "                                           importances[indices[i]]))\n",
    "        \n",
    "    \n",
    "    # Plot Feature Importance (both cumul., individual)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(n), importances[indices[:n]], align='center')\n",
    "    plt.xticks(range(n), feat_labels[indices[:n]], rotation=90)\n",
    "    plt.xlim([-1, n])\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Rel. Feature Importance')\n",
    "    plt.step(range(n), sum_feat_imp, where='mid', \n",
    "         label='Cumulative importance')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    \n",
    "    # Create a list with the important features for ML algorhithms\n",
    "    feature_list = [None] * n\n",
    "    for i in range(n):\n",
    "        feature_list[i] = feat_labels[indices[i]]\n",
    "    \n",
    "    # return the list of important features\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(X_train, Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)\n",
    "    '''\n",
    "    lor = LogisticRegression(max_iter=100, tol=0.001,random_state=1, n_jobs=-1,solver='saga',warm_start=True) #increasing iterations to 1000 increases score by only 1% -> it is not worth the additional time\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('logreg', lor)])\n",
    "\n",
    "    param_grid = {'logreg__penalty': ['elasticnet'], #elastic nets combines l1&l2\n",
    "                  'logreg__C':[6,6.5,7,7.5,8],\n",
    "                  'logreg__l1_ratio':[0,0.05,0.1,0.15,0.2,1]} #if 0, or 1 then l2 or l1 would be best. If between then the combination of both\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train,Y_train)\n",
    "    \n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_poly(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)\n",
    "    '''\n",
    "    \n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_poly', SVC(kernel='poly', random_state=0, max_iter=100000))])\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_poly__C': [900,1000,1100], \n",
    "                  'svm_poly__degree': [3,4,5],\n",
    "                  'svm_poly__gamma': [0,0.05,0.1],\n",
    "                  'svm_poly__coef0':[0.6]}  #Larger gridsearch yielded 0.6 to be the best coef0 with this combination. As it does not greatly change the cv accuracy(<1%) we don't include it in this grid search to lower the computing time.\n",
    "\n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)    \n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000))])\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_rbf__C': [100,150,200], \n",
    "                  'svm_rbf__gamma': [0.25,0.3,0.35]} \n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_rbf_bal(X_train,Y_train):\n",
    "    '''\n",
    "    X_train: Training Set of X values\n",
    "    Y_train: Training Set of Y values(factorized)    \n",
    "    '''\n",
    "\n",
    "    # Create pipeline object with standard scaler and SVC estimator\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('svm_rbf', SVC(kernel='rbf', random_state=0, max_iter=100000, class_weight='balanced'))])\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {'svm_rbf__C': [100,200,300], \n",
    "                  'svm_rbf__gamma': [0.25,0.3,0.35]} \n",
    "    # Run grid search\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=10, n_jobs=-1) #cv=5 yields same accuracy\n",
    "    grid = grid.fit(X_train, Y_train)\n",
    "    return(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
