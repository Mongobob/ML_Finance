{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term Credit Rating Projection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.imputation.mice import MICE, MICEData\n",
    "# import fancyimpute\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "#import all the functions we wrote ourselves\n",
    "import import_ipynb\n",
    "import Functions as functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 3 provided csv and merge them into one Pandas-Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'functions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1baf40478541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0marr3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\SP500_CompanyList.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Combine Keyfigures and S&P500 Company List\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_keyfigures_and_Companies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marr3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#Add Ratings to the mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_Ratings_and_Rest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'functions' is not defined"
     ]
    }
   ],
   "source": [
    "#Add Keyfigures \n",
    "arr1=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\d02dfcb3d4b2bc42.csv')\n",
    "#Add Ratings\n",
    "arr2=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Ratings.csv')\n",
    "#Add S&P500 Company List\n",
    "arr3=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\SP500_CompanyList.csv')\n",
    "#Combine Keyfigures and S&P500 Company List\n",
    "rest=functions.combine_keyfigures_and_Companies(arr1,arr3)\n",
    "#Add Ratings to the mapping\n",
    "mapping=functions.combine_Ratings_and_Rest(rest,arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the joint Dataframe for further use in a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.to_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv', sep=';',\n",
    "                parse_dates=['adate', 'qdate', 'public_date', 'datadate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting all the rows where splticrm has NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['splticrm'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.factorize(df[\"splticrm\"])[0]\n",
    "print(np.bincount(Y))\n",
    "print(pd.factorize(df[\"splticrm\"])[1])\n",
    "Y = pd.DataFrame(Y, columns=[\"Rating as Factor\"])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking a first look at the data we see that for ratings D and CCC we only have 4, respectively 2 observations. Therefore we delete these values due to the very low number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['splticrm'] != 'D']\n",
    "df = df[df['splticrm'] != 'CCC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign to X all the columns but splticrm. Then we drop some columns which will not be relevant for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"splticrm\"]\n",
    "X = X.drop([\"permno\", \"CUSIP\", \"NCUSIP\", \"adate\", \"qdate\", \"public_date\", \"TICKER\"], axis=1)\n",
    "X = X.drop([\"COMNAM\", \"PERMCO\", \"NWPERM\", \"gvkey\", \"datadate\", \"tic\", \"cusip\", \"conm\", \"PRC\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "We decided to drop columns which contain more than 10'000 (corresponds to roughly 30%)  NA values or zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAs = X.isnull().sum() > 10000\n",
    "Zeros =  (X == 0).sum() > 10000\n",
    "delNAs = X.columns[NAs] #drops PEG_trailing\n",
    "delZeros= X.columns[Zeros] #drops rd_sale, adv_sale, staff_sale\n",
    "X = X.drop(delNAs, axis=1)\n",
    "X = X.drop(delZeros, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into a train and a test set. The test set consists of 20% of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are still missing values that have to be imputed. We do this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Imputer\n",
    "The iterative imputer imputes missing values by by modeling each feature containing missing values as a function of other features and is applied separately to the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "## this can be deleted once the functions file is imported\n",
    "\n",
    "num_cols = ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', \n",
    "            'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "            'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "            'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act',\n",
    "            'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct',\n",
    "            'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov',\n",
    "            'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "            'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'accrual', 'ptb',\n",
    "            'DIVYIELD', 'PEG_1yrforward', 'PEG_ltgforward']\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_train_imputed = X_train[num_cols].copy(deep=True)\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "mice_imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "# Impute using fit_tranform on diabetes\n",
    "X_train_imputed.iloc[:, :] = mice_imputer.fit_transform(X_train[num_cols])\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_test_imputed = X_test[num_cols].copy(deep=True)\n",
    "\n",
    "# Impute using tranformation of training set on test set\n",
    "X_test_imputed.iloc[:, :] = mice_imputer.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the latest here import the 'Functions' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "\n",
    "X_train_imputed = my_iterative_imputer(X_train)\n",
    "X_test_imputed = my_iterative_imputer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "To find out which variables are most important we run the 'features_selection' function and select all variables which explain more than 0.15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = feature_selection(x = X_train_imputed, y = y_train.values.ravel(), thres=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset important features matrix for ML algorithms\n",
    "X_train_imputed = X_train_imputed.loc[:,important_features]\n",
    "X_test_imputed = X_test_imputed.loc[:, important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.to_csv('X_train.csv')\n",
    "X_test_imputed.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_train = X_train.iloc[:, 1:]\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "Y_train = pd.read_csv(\"y_train.csv\")\n",
    "Y_test = pd.read_csv(\"y_test.csv\")\n",
    "Y_train = Y_train['Rating as Factor'].astype('category') #factorize trainset\n",
    "Y_test = Y_test['Rating as Factor'].astype('category')   #factorize testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "grid = LogReg(X_train,Y_train)\n",
    "print('Best parameters:', grid.best_params_) #best parameters are C=7 & ratio=0 -> l2 penalty function\n",
    "print('Best CV accuracy:', grid.best_score_)\n",
    "print('Test score:', grid.score(X_test,Y_test)) #31%\n",
    "print(datetime.datetime.now()) #10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Polynomial Kernel Function#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "poly = SVM_poly(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(poly.best_score_))\n",
    "print('Test score:       {:.2f}'.format(poly.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(poly.best_params_))\n",
    "print(datetime.datetime.now()) #20min\n",
    "\n",
    "# Predict classes\n",
    "y_pred = poly.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf)#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf = SVM_rbf(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(rbf.best_score_))\n",
    "print('Test score:       {:.2f}'.format(rbf.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf.best_params_))\n",
    "print(datetime.datetime.now()) #10min\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf) with Balanced class weights#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf_bal = SVM_rbf_bal(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(rbf_bal.best_score_))\n",
    "print('Test score:       {:.2f}'.format(rbf_bal.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf_bal.best_params_))\n",
    "print(datetime.datetime.now())#10min\n",
    "\n",
    "#looking at the confusion matrix of non-balanced rbf we see that the smaller classes don't get more wrong classification. Therefore balancing the weights should not influence the outcome greatly which it doesn't\n",
    "\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf_bal.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel with Balanced class weights yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "#According to \"https://stackoverflow.com/questions/21390570/scikit-learn-svc-coef0-parameter-range\" the Sigmoid function does not fulfill the definition of a kernel as it is not positive semidefinite. Therefore we will not use it with Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
