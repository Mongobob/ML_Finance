{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term Credit Rating Projection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.imputation.mice import MICE, MICEData\n",
    "# import fancyimpute\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "#import all the functions we wrote ourselves\n",
    "import import_ipynb\n",
    "import Functions as functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 3 provided csv and merge them into one Pandas-Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Keyfigures \n",
    "arr1=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\d02dfcb3d4b2bc42.csv')\n",
    "#Add Ratings\n",
    "arr2=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Ratings.csv')\n",
    "#Add S&P500 Company List\n",
    "arr3=pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\SP500_CompanyList.csv')\n",
    "#Combine Keyfigures and S&P500 Company List\n",
    "rest=functions.combine_keyfigures_and_Companies(arr1,arr3)\n",
    "#Add Ratings to the mapping\n",
    "mapping=functions.combine_Ratings_and_Rest(rest,arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the joint Dataframe for further use in a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping.to_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv', sep=';',\n",
    "                #parse_dates=['adate', 'qdate', 'public_date', 'datadate'])\n",
    "df = mapping\n",
    "df=df.rename(columns={'public_date_x':'public_date'})\n",
    "df=df.rename(columns={'TICKER_x':'TICKER'})\n",
    "df=df.rename(columns={'TICKER_y':'tic'})\n",
    "df= df.drop(['public_date_y'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting all the rows where splticrm has NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['splticrm'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3656 1943  990  296 5455 3209  489 3882 4682  199   50  143  382 1069\n",
      " 1648 1096  220    4    2]\n",
      "Index(['A', 'A+', 'AA-', 'AAA', 'BBB', 'BBB-', 'AA', 'A-', 'BBB+', 'AA+',\n",
      "       'CCC+', 'B-', 'B+', 'BB-', 'BB+', 'BB', 'B', 'D', 'CCC'],\n",
      "      dtype='object')\n",
      "       Rating as Factor\n",
      "0                     0\n",
      "1                     0\n",
      "2                     0\n",
      "3                     0\n",
      "4                     0\n",
      "...                 ...\n",
      "29410                 8\n",
      "29411                 8\n",
      "29412                 8\n",
      "29413                 8\n",
      "29414                 8\n",
      "\n",
      "[29415 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "Y = pd.factorize(df[\"splticrm\"])[0]\n",
    "print(np.bincount(Y))\n",
    "print(pd.factorize(df[\"splticrm\"])[1])\n",
    "Y = pd.DataFrame(Y, columns=[\"Rating as Factor\"])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking a first look at the data we see that for ratings D and CCC we only have 4, respectively 2 observations. Therefore we delete these values due to the very low number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   permno       adate     qdate public_date   CAPEI     bm     evm  \\\n",
      "0   10104  20090531.0  20091130  2010-01-31  26.386  0.252  10.110   \n",
      "1   10104  20090531.0  20091130  2010-02-28  28.226  0.252  10.110   \n",
      "2   10104  20090531.0  20091130  2010-03-31  29.464  0.252  10.110   \n",
      "3   10104  20090531.0  20100228  2010-04-30  28.783  0.233  10.983   \n",
      "4   10104  20090531.0  20100228  2010-05-31  25.096  0.233  10.983   \n",
      "\n",
      "   pe_op_basic  pe_op_dil  pe_exi  ...     CUSIP  NWPERM    PRC  \\\n",
      "0       19.217     19.378  20.052  ...  68389X10       0  44.06   \n",
      "1       20.542     20.714  21.435  ...  68389X10       0  44.06   \n",
      "2       21.425     21.605  22.357  ...  68389X10       0  44.06   \n",
      "3       21.378     21.556  23.096  ...  68389X10       0  44.06   \n",
      "4       18.653     18.808  20.152  ...  68389X10       0  44.06   \n",
      "\n",
      "              key    gvkey  splticrm    datadate         conm   tic      cusip  \n",
      "0  2010-01-31ORCL  12142.0         A  20100131.0  ORACLE CORP  ORCL  68389X105  \n",
      "1  2010-02-28ORCL  12142.0         A  20100228.0  ORACLE CORP  ORCL  68389X105  \n",
      "2  2010-03-31ORCL  12142.0         A  20100331.0  ORACLE CORP  ORCL  68389X105  \n",
      "3  2010-04-30ORCL  12142.0         A  20100430.0  ORACLE CORP  ORCL  68389X105  \n",
      "4  2010-05-31ORCL  12142.0         A  20100531.0  ORACLE CORP  ORCL  68389X105  \n",
      "\n",
      "[5 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df[df['splticrm'] != 'D']\n",
    "df = df[df['splticrm'] != 'CCC']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign to X all the columns but splticrm. Then we drop some columns which will not be relevant for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAPEI</th>\n",
       "      <th>bm</th>\n",
       "      <th>evm</th>\n",
       "      <th>pe_op_basic</th>\n",
       "      <th>pe_op_dil</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>pe_inc</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>...</th>\n",
       "      <th>adv_sale</th>\n",
       "      <th>staff_sale</th>\n",
       "      <th>accrual</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>PEG_1yrforward</th>\n",
       "      <th>PEG_ltgforward</th>\n",
       "      <th>date</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.386</td>\n",
       "      <td>0.252</td>\n",
       "      <td>10.110</td>\n",
       "      <td>19.217</td>\n",
       "      <td>19.378</td>\n",
       "      <td>20.052</td>\n",
       "      <td>20.052</td>\n",
       "      <td>4.976</td>\n",
       "      <td>13.353</td>\n",
       "      <td>0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>4.145</td>\n",
       "      <td>1.045</td>\n",
       "      <td>.867%</td>\n",
       "      <td>2.221</td>\n",
       "      <td>1.554</td>\n",
       "      <td>29/06/2018</td>\n",
       "      <td>2010-01-31ORCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.226</td>\n",
       "      <td>0.252</td>\n",
       "      <td>10.110</td>\n",
       "      <td>20.542</td>\n",
       "      <td>20.714</td>\n",
       "      <td>21.435</td>\n",
       "      <td>21.435</td>\n",
       "      <td>5.323</td>\n",
       "      <td>14.285</td>\n",
       "      <td>0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>4.434</td>\n",
       "      <td>1.117</td>\n",
       "      <td>.811%</td>\n",
       "      <td>2.058</td>\n",
       "      <td>1.520</td>\n",
       "      <td>29/06/2018</td>\n",
       "      <td>2010-02-28ORCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.464</td>\n",
       "      <td>0.252</td>\n",
       "      <td>10.110</td>\n",
       "      <td>21.425</td>\n",
       "      <td>21.605</td>\n",
       "      <td>22.357</td>\n",
       "      <td>22.357</td>\n",
       "      <td>5.556</td>\n",
       "      <td>14.911</td>\n",
       "      <td>0.129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>4.628</td>\n",
       "      <td>1.165</td>\n",
       "      <td>.778%</td>\n",
       "      <td>2.146</td>\n",
       "      <td>1.586</td>\n",
       "      <td>29/06/2018</td>\n",
       "      <td>2010-03-31ORCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.783</td>\n",
       "      <td>0.233</td>\n",
       "      <td>10.983</td>\n",
       "      <td>21.378</td>\n",
       "      <td>21.556</td>\n",
       "      <td>23.096</td>\n",
       "      <td>23.096</td>\n",
       "      <td>5.381</td>\n",
       "      <td>15.909</td>\n",
       "      <td>0.177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049</td>\n",
       "      <td>4.515</td>\n",
       "      <td>1.545</td>\n",
       "      <td>.773%</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1.642</td>\n",
       "      <td>29/06/2018</td>\n",
       "      <td>2010-04-30ORCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.096</td>\n",
       "      <td>0.233</td>\n",
       "      <td>10.983</td>\n",
       "      <td>18.653</td>\n",
       "      <td>18.808</td>\n",
       "      <td>20.152</td>\n",
       "      <td>20.152</td>\n",
       "      <td>4.692</td>\n",
       "      <td>13.871</td>\n",
       "      <td>0.177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049</td>\n",
       "      <td>3.937</td>\n",
       "      <td>1.348</td>\n",
       "      <td>.886%</td>\n",
       "      <td>1.612</td>\n",
       "      <td>1.432</td>\n",
       "      <td>29/06/2018</td>\n",
       "      <td>2010-05-31ORCL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CAPEI     bm     evm  pe_op_basic  pe_op_dil  pe_exi  pe_inc     ps  \\\n",
       "0  26.386  0.252  10.110       19.217     19.378  20.052  20.052  4.976   \n",
       "1  28.226  0.252  10.110       20.542     20.714  21.435  21.435  5.323   \n",
       "2  29.464  0.252  10.110       21.425     21.605  22.357  22.357  5.556   \n",
       "3  28.783  0.233  10.983       21.378     21.556  23.096  23.096  5.381   \n",
       "4  25.096  0.233  10.983       18.653     18.808  20.152  20.152  4.692   \n",
       "\n",
       "      pcf    dpr  ...  adv_sale  staff_sale  accrual    ptb  PEG_trailing  \\\n",
       "0  13.353  0.129  ...     0.003         0.0    0.058  4.145         1.045   \n",
       "1  14.285  0.129  ...     0.003         0.0    0.058  4.434         1.117   \n",
       "2  14.911  0.129  ...     0.003         0.0    0.058  4.628         1.165   \n",
       "3  15.909  0.177  ...     0.003         0.0    0.049  4.515         1.545   \n",
       "4  13.871  0.177  ...     0.003         0.0    0.049  3.937         1.348   \n",
       "\n",
       "   DIVYIELD  PEG_1yrforward  PEG_ltgforward        date             key  \n",
       "0     .867%           2.221           1.554  29/06/2018  2010-01-31ORCL  \n",
       "1     .811%           2.058           1.520  29/06/2018  2010-02-28ORCL  \n",
       "2     .778%           2.146           1.586  29/06/2018  2010-03-31ORCL  \n",
       "3     .773%           1.848           1.642  29/06/2018  2010-04-30ORCL  \n",
       "4     .886%           1.612           1.432  29/06/2018  2010-05-31ORCL  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.loc[:, df.columns != \"splticrm\"]\n",
    "X = X.drop([\"permno\", \"CUSIP\", \"NCUSIP\", \"adate\", \"qdate\", \"public_date\", \"TICKER\"], axis=1)\n",
    "X = X.drop([\"COMNAM\", \"PERMCO\", \"NWPERM\", \"gvkey\", \"datadate\", \"tic\", \"cusip\", \"conm\", \"PRC\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "We decided to drop columns which contain more than 10'000 (corresponds to roughly 30%)  NA values or zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAs = X.isnull().sum() > 10000\n",
    "Zeros =  (X == 0).sum() > 10000\n",
    "delNAs = X.columns[NAs] #drops PEG_trailing\n",
    "delZeros= X.columns[Zeros] #drops rd_sale, adv_sale, staff_sale\n",
    "X = X.drop(delNAs, axis=1)\n",
    "X = X.drop(delZeros, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into a train and a test set. The test set consists of 20% of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [29409, 29415]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5041eff5b8d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                     \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                     stratify=Y)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\sandro\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2116\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2118\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandro\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sandro\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 212\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [29409, 29415]"
     ]
    }
   ],
   "source": [
    "#now do the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are still missing values that have to be imputed. We do this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Imputer\n",
    "The iterative imputer imputes missing values by by modeling each feature containing missing values as a function of other features and is applied separately to the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "## this can be deleted once the functions file is imported\n",
    "\n",
    "num_cols = ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', \n",
    "            'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "            'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "            'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act',\n",
    "            'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct',\n",
    "            'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov',\n",
    "            'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "            'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'accrual', 'ptb',\n",
    "            'DIVYIELD', 'PEG_1yrforward', 'PEG_ltgforward']\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_train_imputed = X_train[num_cols].copy(deep=True)\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "mice_imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "# Impute using fit_tranform on diabetes\n",
    "X_train_imputed.iloc[:, :] = mice_imputer.fit_transform(X_train[num_cols])\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_test_imputed = X_test[num_cols].copy(deep=True)\n",
    "\n",
    "# Impute using tranformation of training set on test set\n",
    "X_test_imputed.iloc[:, :] = mice_imputer.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the latest here import the 'Functions' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "\n",
    "X_train_imputed = my_iterative_imputer(X_train)\n",
    "X_test_imputed = my_iterative_imputer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "To find out which variables are most important we run the 'features_selection' function and select all variables which explain more than 0.15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = feature_selection(x = X_train_imputed, y = y_train.values.ravel(), thres=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset important features matrix for ML algorithms\n",
    "X_train_imputed = X_train_imputed.loc[:,important_features]\n",
    "X_test_imputed = X_test_imputed.loc[:, important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.to_csv('X_train.csv')\n",
    "X_test_imputed.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_train = X_train.iloc[:, 1:]\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "Y_train = pd.read_csv(\"y_train.csv\")\n",
    "Y_test = pd.read_csv(\"y_test.csv\")\n",
    "Y_train = Y_train['Rating as Factor'].astype('category') #factorize trainset\n",
    "Y_test = Y_test['Rating as Factor'].astype('category')   #factorize testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "grid = LogReg(X_train,Y_train)\n",
    "print('Best parameters:', grid.best_params_) #best parameters are C=7 & ratio=0 -> l2 penalty function\n",
    "print('Best CV accuracy:', grid.best_score_)\n",
    "print('Test score:', grid.score(X_test,Y_test)) #31%\n",
    "print(datetime.datetime.now()) #10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Polynomial Kernel Function#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "poly = SVM_poly(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(poly.best_score_))\n",
    "print('Test score:       {:.2f}'.format(poly.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(poly.best_params_))\n",
    "print(datetime.datetime.now()) #20min\n",
    "\n",
    "# Predict classes\n",
    "y_pred = poly.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf)#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf = SVM_rbf(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(rbf.best_score_))\n",
    "print('Test score:       {:.2f}'.format(rbf.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf.best_params_))\n",
    "print(datetime.datetime.now()) #10min\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf) with Balanced class weights#######\n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf_bal = SVM_rbf_bal(X_train,Y_train)\n",
    "print('Best CV accuracy: {:.2f}'.format(rbf_bal.best_score_))\n",
    "print('Test score:       {:.2f}'.format(rbf_bal.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf_bal.best_params_))\n",
    "print(datetime.datetime.now())#10min\n",
    "\n",
    "#looking at the confusion matrix of non-balanced rbf we see that the smaller classes don't get more wrong classification. Therefore balancing the weights should not influence the outcome greatly which it doesn't\n",
    "\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf_bal.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel with Balanced class weights yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "#According to \"https://stackoverflow.com/questions/21390570/scikit-learn-svc-coef0-parameter-range\" the Sigmoid function does not fulfill the definition of a kernel as it is not positive semidefinite. Therefore we will not use it with Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
