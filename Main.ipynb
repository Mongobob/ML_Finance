{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term Credit Rating Projection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3e71c3d980bc>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-3e71c3d980bc>\"\u001b[1;36m, line \u001b[1;32m49\u001b[0m\n\u001b[1;33m    import Moritz Functions as functions\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.imputation.mice import MICE, MICEData\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.constraints import maxnorm\n",
    "from matplotlib import pyplot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "# import fancyimpute\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "#import all the functions we wrote ourselves\n",
    "import import_ipynb\n",
    "import Moritz Functions as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 3 provided csv and merge them into one Pandas-Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Keyfigures \n",
    "arr1=pd.read_csv(\"Keyfigures.csv\")\n",
    "#Add Ratings\n",
    "arr2=pd.read_csv(\"Ratings.csv\")\n",
    "#Add S&P500 Company List\n",
    "arr3=pd.read_csv(\"SP500_CompanyList.csv\")\n",
    "#Combine Keyfigures and S&P500 Company List\n",
    "rest=functions.combine_keyfigures_and_Companies(arr1,arr3)\n",
    "#Add Ratings to the mapping\n",
    "mapping=functions.combine_Ratings_and_Rest(rest,arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the joint Dataframe for further use in a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping.to_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Data\\Clean Mapping.csv', sep=';',\n",
    "                #parse_dates=['adate', 'qdate', 'public_date', 'datadate'])\n",
    "df = mapping\n",
    "df=df.rename(columns={'public_date_x':'public_date'})\n",
    "df=df.rename(columns={'TICKER_x':'TICKER'})\n",
    "df=df.rename(columns={'TICKER_y':'tic'})\n",
    "df= df.drop(['public_date_y'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting all the rows where splticrm has NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['splticrm'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['splticrm'] != 'D']\n",
    "df = df[df['splticrm'] != 'CCC']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.factorize(df[\"splticrm\"])[0]\n",
    "print(np.bincount(Y))\n",
    "print(pd.factorize(df[\"splticrm\"])[1])\n",
    "Y = pd.DataFrame(Y, columns=[\"Rating as Factor\"])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking a first look at the data we see that for ratings D and CCC we only have 4, respectively 2 observations. Therefore we delete these values due to the very low number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "We decided to drop columns which contain more than 10'000 (corresponds to roughly 30%)  NA values or zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a81aa5fd74e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNAs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mZeros\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdelNAs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNAs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#drops PEG_trailing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdelZeros\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mZeros\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#drops rd_sale, adv_sale, staff_sale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelNAs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "NAs = df.isnull().sum() > 10000\n",
    "Zeros =  (df == 0).sum() > 10000\n",
    "delNAs = df.columns[NAs] #drops PEG_trailing\n",
    "delZeros= df.columns[Zeros] #drops rd_sale, adv_sale, staff_sale\n",
    "df = df.drop(delNAs, axis=1)\n",
    "df = df.drop(delZeros, axis=1)\n",
    "df.to_csv(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Collection\\ML_Finance-master\\Clean Mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column of Ratings which are lagging by 1 Month and merge it with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nest_list=functions.create_lagging_Ratings_1M(r'C:\\Users\\Sandro\\OneDrive\\Dokumente\\Universität Zürich\\Aufbaustufe\\2020 FS\\Introduction to Machine Learning\\Group Project\\Collection\\ML_Finance-master\\Clean Mapping.csv')\n",
    "dframe=pd.DataFrame(nest_list,columns=['key','Lagging-Rating_1M'])\n",
    "#Replace the Nonetype from python with NaN from pandas\n",
    "dframe.fillna(value=pd.np.nan, inplace=True)\n",
    "#Now merge the new Dataframe to our main-df\n",
    "df = df.merge(dframe, how='left', on=['key'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign to X all the columns but splticrm. Then we drop some columns which will not be relevant for the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"splticrm\"]\n",
    "X = X.drop([\"permno\", \"CUSIP\", \"NCUSIP\", \"adate\", \"qdate\", \"public_date\", \"TICKER\"], axis=1)\n",
    "X = X.drop([\"COMNAM\", \"PERMCO\", \"gvkey\", \"datadate\", \"tic\", \"cusip\", \"conm\", \"PRC\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "We decided to drop columns which contain more than 10'000 (corresponds to roughly 30%)  NA values or zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into a train and a test set. The test set consists of 20% of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5041eff5b8d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#now do the train test split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(X, Y, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                                     \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                     stratify=Y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#now do the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are still missing values that have to be imputed. We do this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Imputer\n",
    "The iterative imputer imputes missing values by by modeling each feature containing missing values as a function of other features and is applied separately to the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "## this can be deleted once the functions file is imported\n",
    "\n",
    "num_cols = ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', \n",
    "            'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq',\n",
    "            'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "            'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act',\n",
    "            'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct',\n",
    "            'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov',\n",
    "            'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "            'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'accrual', 'ptb',\n",
    "            'DIVYIELD', 'PEG_1yrforward', 'PEG_ltgforward']\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_train_imputed = X_train[num_cols].copy(deep=True)\n",
    "\n",
    "#Remove \"%\" from the DIVYIELD-Col\n",
    "X_train_imputed['DIVYIELD'] = X_train_imputed['DIVYIELD'].str.rstrip('%').astype('float') / 100.0\n",
    "#X_train_imputed.to_csv(\"Zwischencheck.csv\")\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "#mice_imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "# Impute using fit_tranform on diabetes\n",
    "#X_train_imputed.iloc[:, :] = mice_imputer.fit_transform(X_train_imputed[num_cols])\n",
    "\n",
    "# Copy df to df_mice_imputed\n",
    "X_test_imputed = X_test[num_cols].copy(deep=True)\n",
    "X_test_imputed['DIVYIELD'] = X_test_imputed['DIVYIELD'].str.rstrip('%').astype('float') / 100.0\n",
    "\n",
    "# Impute using tranformation of training set on test set\n",
    "#X_test_imputed.iloc[:, :] = mice_imputer.transform(X_test_imputed[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply IterativeImputer\n",
    "\n",
    "X_train_imputed = functions.my_iterative_imputer(X_train_imputed)\n",
    "X_test_imputed = functions.my_iterative_imputer(X_test_imputed)\n",
    "X_train_imputed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train and test data with lagging ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save lagging ratings\n",
    "lag_rating_train = X_train[\"Lagging-Rating_1M\"]\n",
    "lag_rating_test = X_test[\"Lagging-Rating_1M\"]\n",
    "\n",
    "#Merge imputed data with lagging ratings\n",
    "X_train_imputed_lag = X_train_imputed.copy()\n",
    "X_test_imputed_lag = X_test_imputed.copy()\n",
    "\n",
    "X_train_imputed_lag[\"Lagging-Rating_1M\"] = lag_rating_train\n",
    "X_test_imputed_lag[\"Lagging-Rating_1M\"] = lag_rating_test\n",
    "\n",
    "#Define indetificator for train/test set in order to combine, modify and separate them again\n",
    "X_train_imputed_lag[\"train\"] = 1\n",
    "X_test_imputed_lag[\"train\"] = 0\n",
    "\n",
    "#Combine both to factorize lagged ratings together\n",
    "combined = pd.concat([X_train_imputed_lag, X_test_imputed_lag])\n",
    "\n",
    "#Drop rows with NA in lagged rating\n",
    "combined = combined[combined[\"Lagging-Rating_1M\"].notna()]\n",
    "combined[\"Lagging-Rating_1M\"] = pd.factorize(combined[\"Lagging-Rating_1M\"])[0]\n",
    "\n",
    "#Separate train and test set\n",
    "X_train_imputed_lag = combined[combined[\"train\"] == 1]\n",
    "X_test_imputed_lag = combined[combined[\"train\"] == 0]\n",
    "X_train_imputed_lag.drop([\"train\"], axis=1, inplace=True)\n",
    "X_test_imputed_lag.drop([\"train\"], axis=1, inplace=True)\n",
    "\n",
    "#Get the lagging ratings Dataframes Y for train and test set\n",
    "y_train_lag = X_train_imputed_lag[\"Lagging-Rating_1M\"]\n",
    "y_test_lag = X_test_imputed_lag[\"Lagging-Rating_1M\"]\n",
    "\n",
    "y_train_lag = pd.DataFrame(y_train_lag)\n",
    "y_test_lag = pd.DataFrame(y_test_lag)\n",
    "\n",
    "#Drop Lagging Raating out of imputed feature matrix\n",
    "X_train_imputed_lag.drop([\"Lagging-Rating_1M\"], axis=1, inplace=True)\n",
    "X_test_imputed_lag.drop([\"Lagging-Rating_1M\"], axis=1, inplace=True)\n",
    "\n",
    "y_train_lag.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "#### For ordinary ratings\n",
    "To find out which variables are most important we run the 'features_selection' function and select all variables which explain more than 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = functions.feature_selection(x = X_train_imputed, y = y_train.values.ravel(), thres=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset important features matrix for ML algorithms\n",
    "X_train_imputed = X_train_imputed.loc[:,important_features]\n",
    "X_test_imputed = X_test_imputed.loc[:, important_features]\n",
    "X_train_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.to_csv('X_train.csv')\n",
    "X_test_imputed.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For lagging ratings\n",
    "Same approach as before. We select all the variables which explain more than 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = functions.feature_selection(x = X_train_imputed_lag, y = y_train_lag.values.ravel(), thres=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset important features matrix for ML algorithms\n",
    "X_train_imputed_lag = X_train_imputed_lag.loc[:,important_features]\n",
    "X_test_imputed_lag = X_test_imputed_lag.loc[:, important_features]\n",
    "X_train_imputed_lag.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed_lag.to_csv('X_train_lag.csv')\n",
    "X_test_imputed_lag.to_csv('X_test_lag.csv')\n",
    "y_train_lag.to_csv('y_train_lag.csv')\n",
    "y_test_lag.to_csv('y_test_lag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "## Ordinary Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_train = X_train.iloc[:, 1:]\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "Y_train = pd.read_csv(\"y_train.csv\")\n",
    "Y_test = pd.read_csv(\"y_test.csv\")\n",
    "Y_train = Y_train['Rating as Factor'].astype('category') #factorize trainset\n",
    "Y_test = Y_test['Rating as Factor'].astype('category')   #factorize testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the grid of logreg__C:[6,6.5,7,7.5,8] and logreg__l1_ratio:[0,0.05,0.1,0.15,0.2,1] the best parameters were: {'logreg__C': 7, 'logreg__l1_ratio': 0, 'logreg__penalty': 'elasticnet'}\n",
    "\n",
    "A ratio of 0 means that we are using the l2 penalty function (ridge regression regularization and not lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "param_grid = {'logreg__penalty': ['elasticnet'], #elastic nets combines l1&l2\n",
    "              'logreg__C':[7],\n",
    "              'logreg__l1_ratio':[0]} #if 0, or 1 then l2 or l1 would be best. If between then the combination of both\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "grid = functions.LogReg(X_train,Y_train,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(grid.best_score_)) #best parameters are C=7 & ratio=0 -> l2 penalty function\n",
    "print('Test score:       {:.4f}'.format(grid.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(grid.best_params_)) #31%\n",
    "print(datetime.datetime.now()) #10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial Kernel function: \n",
    "The gridsearch over {'svm_poly__C': [900,1000,1100], 'svm_poly__degree': [3,4,5], 'svm_poly__gamma': [0,0.05,0.1], 'svm_poly__coef0':[0.6]} \n",
    "resulted in {'svm_poly__C': 1000, 'svm_poly__coef0': 0.6, 'svm_poly__degree': 4, 'svm_poly__gamma': 0.05} being the best parameters. As the values are not at the gridsearch boundary we found a minima and can therefore stop the gridsearch. \n",
    "\n",
    "(Larger gridsearch beforehand yielded 0.6 to be the best coef0. As it does not greatly change the cv accuracy(<1%) we didn't include it in this grid search to lower the computing time.)\n",
    "\n",
    "##### Radial Basis Kernel Function(rbf):\n",
    "The gridsearch over {'svm_rbf__C': [100,150,200], 'svm_rbf__gamma': [0.25,0.3,0.35]} (and larger gridsearches beforehand) resulted in {'svm_rbf__C': 150, 'svm_rbf__gamma': 0.3} being the best parameters.\n",
    "\n",
    "##### Radial Basis Kernel Function(rbf) with Balanced class weights:\n",
    "The gridsearch over {'svm_rbf__C': [100,200,300], 'svm_rbf__gamma': [0.25,0.3,0.35]}  resulted in {'svm_rbf__C': 200, 'svm_rbf__gamma': 0.3} being the best parameters.\n",
    "\n",
    "(According to \"https://stackoverflow.com/questions/21390570/scikit-learn-svc-coef0-parameter-range\" the Sigmoid function does not fulfill the definition of a kernel as it is not positive semidefinite. Therefore we will not use it with Support Vector Machines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Polynomial Kernel Function#######\n",
    "param_grid = {'svm_poly__C': [1000], \n",
    "              'svm_poly__degree': [4],\n",
    "              'svm_poly__gamma': [0.05],\n",
    "              'svm_poly__coef0':[0.6]}\n",
    "    \n",
    "print(datetime.datetime.now()) #computation time\n",
    "poly = functions.SVM_poly(X_train,Y_train, param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(poly.best_score_))\n",
    "print('Test score:       {:.4f}'.format(poly.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(poly.best_params_))\n",
    "print(datetime.datetime.now()) \n",
    "\n",
    "# Predict classes\n",
    "y_pred = poly.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf)#######\n",
    "param_grid = {'svm_rbf__C': [150], \n",
    "              'svm_rbf__gamma': [0.3]} \n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf = functions.SVM_rbf(X_train,Y_train,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(rbf.best_score_))\n",
    "print('Test score:       {:.4f}'.format(rbf.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf) with Balanced class weights#######\n",
    "param_grid = {'svm_rbf__C': [200], \n",
    "              'svm_rbf__gamma': [0.3]} \n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf_bal = functions.SVM_rbf_bal(X_train,Y_train,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(rbf_bal.best_score_))\n",
    "print('Test score:       {:.4f}'.format(rbf_bal.score(X_test, Y_test)))\n",
    "print('Best parameters: {}'.format(rbf_bal.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf_bal.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Radial Basis Function Kernel with Balanced class weights yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix of non-balanced rbf we see that the smaller classes don't get more wrong classification. Therefore balancing the weights should not influence the outcome greatly which it doesn't\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define arrays of values to be tested for paramgrid inside GridSearchCV function\n",
    "\n",
    "#number of trees in the forest\n",
    "n_estimators = np.array([70])\n",
    "\n",
    "# Max depth\n",
    "maxDepth = np.array([25])\n",
    "\n",
    "# Minimum number of samples required to split any internal node \n",
    "minSamplesNode = np.array([2])\n",
    "\n",
    "# The minimum number of samples required to be at a leaf/terminal node\n",
    "minSamplesLeaf = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RandomForest Classifier\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "forest = functions.random_forest(X_train,Y_train, n_estimators, maxDepth, minSamplesNode, minSamplesLeaf)\n",
    "print('Best CV accuracy: {:.4f}'.format(forest.best_score_))\n",
    "print('Test score:       {:.4f}'.format(forest.score(X_test,Y_test)))\n",
    "print('Best parameters: {}'.format(forest.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define arrays of values to be tested for paramgrid inside GridSearchCV function\n",
    "\n",
    "#The ith element represents the number of neurons in the ith hidden layer.\n",
    "hidden_lay = np.array([400])\n",
    "\n",
    "# Max number of iterations\n",
    "max_Iter = np.array([575])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run MLP Classifier\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "mlp = functions.neural(X_train,Y_train, hidden_lay, max_Iter)\n",
    "print('Best CV accuracy: {:.4f}'.format(mlp.best_score_))\n",
    "print('Test score:       {:.4f}'.format(mlp.score(X_test,Y_test)))\n",
    "print('Best parameters: {}'.format(mlp.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pipelining a grid search doesn't work with the keras wrapper for sklearn we first have to manually standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train) #standardizing using StandardScaler\n",
    "X_test_std = scaler.transform(X_test) #standardizing using StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best Grid Search would be to test all hyperparameter configurations simultaneously because of possible interactions of the hyperparameters. \n",
    "However this would take an extremely long time due to the many hyperparameters and their possible values.\n",
    "Therefore we take a sequential approach(1 or 2 hyperparameters at a time) on grid searching the hyperparameters to not have to train the model over multiple days.\n",
    "\n",
    "##### Steps of grid search for hyperparameters:\n",
    "After 100 epochs the accuracy doesn't improve much therefore we use epoch = 100 in order to have a well trained neural net which doesn't need too much time to fit.\n",
    "\n",
    "batch_size = [5,10,20] yields that 5 is the best hyperparameter. It makes sense that a lower batch size results in a better accuracy since it optimizes more often. \n",
    "A lower batch size means we more often adjust the weights which takes time. \n",
    "The differences between the batch size accuracy is only 1% while the time for computing is significantly lower for higher batch sizes. \n",
    "Therefore for finding the other optimal hyperparameters we use a batch size of 20 and might adjust it for the final model fitting with the optimal hyperparameters\n",
    "\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'] yielded Nadam to be the best optimizer with Adam on second rank.\n",
    "Nadam is is Adam with Nesterov Momentum as they both take the same amount of time to train we'll use Nadam because of its better accuracy.\n",
    "\n",
    "Next up we split our gridsearch into a single layered and a two layered neural network.\n",
    "\n",
    "##### One layer:\n",
    "For the neural net with one layer we try to find the best combination of activation function and neurons as they interact heavily: Best combination of each activation function for neurons in grid[150,200,250,300]: \n",
    "\n",
    "Accuracy (Standard devation): 0.893909 (0.010722) with: {'activ': 'relu', 'neurons': 300}\n",
    "\n",
    "Accuracy (Standard devation): 0.937349 (0.001372) with: {'activ': 'tanh', 'neurons': 250}\n",
    "\n",
    "Accuracy (Standard devation): 0.933268 (0.000530) with: {'activ': 'sigmoid', 'neurons': 250\n",
    "\n",
    "Accuracy (Standard devation): 0.897989 (0.006316) with: {'activ': 'hard_sigmoid', 'neurons': 300}\n",
    "\n",
    "Accuracy (Standard devation): 0.767458 (0.032144) with: {'activ': 'softmax', 'neurons': 300}\n",
    "\n",
    "Accuracy (Standard devation): 0.846942 (0.002463) with: {'activ': 'softplus', 'neurons': 300}\n",
    "\n",
    "Accuracy (Standard devation): 0.935521 (0.003249) with: {'activ': 'softsign', 'neurons': 250}\n",
    "\n",
    "Accuracy (Standard devation): 0.315170 (0.006817) with: {'activ': 'linear', 'neurons': 300}\n",
    "\n",
    "We now test the Top 3 activation functions for 200 epochs with neurons grid[250,300,350,400] in hopes that maybe after 200 epochs one activation function will be clearly better:\n",
    "\n",
    "Best is the Accuracy 0.947295 (0.000936) with: {'activ': 'sigmoid', 'neurons': 350} (1% better than other activation functions)\n",
    "\n",
    "Next up we checked the dropoutrate combined with a weight restraint. The resulting best hyperparameters for the dropout rate were 0.0 and for the weight constraint the highest possible value. This means that regularization here is not needed and we shouldn't use dropouts or a weight constraint.\n",
    "Checking dropoutrate together with weight constraint we get a dropout rate of 0.0 and a high weight constraint as best hyperparameters.\n",
    "This means regularization here does not help and we don't use dropouts or weight constraints. We now have our final model.\n",
    "\n",
    "##### Two layers:\n",
    "For the two layered neural net we test the possible combinations of neurons with different activation functions in each layer .\n",
    "We only included softsign, relu and sigmoid as the possible activation function such that the fitting of the net doesn't take days. We chose these 3 activation functions because sigmoid&softsign performed very well on the single layered neural net and relu seems to be favoured by a lot of people in the literature.\n",
    "We didn't include tanh because it seems to work very similar to the sigmoid acivation function. \n",
    "\n",
    "Grid Search found this as the best combination: Accuracy: 0.943087 using {'activ1': 'sigmoid', 'activ2': 'sigmoid', 'neurons1': 400, 'neurons2': 150}\n",
    "\n",
    "Again the grid search found that a dropout rate of 0.0 is the best combination in both layers. We now finished the two layered neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1Layer Neural Net(According to the Universal Approximation Theorem theoretically a 1 Layer Neural Net can approximate any other Neural Network):\n",
    "#Final model after Hyperparameter optimization. We get a 96% accuracy for the testset\n",
    "dropout_rate = [0.0]\n",
    "optimizer = ['Nadam']\n",
    "activ = ['sigmoid']\n",
    "neurons = [350]\n",
    "epochs=[200]\n",
    "batch_size=[10]\n",
    "print(datetime.datetime.now())\n",
    "# model fitting\n",
    "\n",
    "param_grid = dict(dropout_rate=dropout_rate, optimizer=optimizer,activ=activ,neurons=neurons,epochs=epochs,batch_size=batch_size)\n",
    "grid = functions.NeuralNet1layer(X_train_std,Y_train,param_grid)\n",
    "#results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Test score:', grid.score(X_test_std,Y_test)) \n",
    "y_pred = grid.predict(X_test_std)\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the optimal Neural Network with 1 Layer on the whole Training Data\n",
    "model = Sequential()\n",
    "model.add(Dense(350, input_dim=36, activation='sigmoid'))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(X_train_std, Y_train, validation_data=(X_test_std, Y_test), epochs=200, batch_size=10, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2Layer Neural Net:\n",
    "activ1 = ['sigmoid']\n",
    "activ2 = ['sigmoid']\n",
    "neurons1 = [400]\n",
    "neurons2 = [150]\n",
    "dropout_rate1 = [0.0]\n",
    "dropout_rate2 = [0.0]\n",
    "print(datetime.datetime.now())\n",
    "# model fitting\n",
    "\n",
    "param_grid = dict(activ1=activ1,activ2=activ2,neurons1=neurons1,neurons2=neurons2,dropout_rate1=dropout_rate1,dropout_rate2=dropout_rate2)\n",
    "grid = functions.NeuralNet2layer(X_train_std,Y_train,param_grid)\n",
    "#results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Test score:', grid.score(X_test_std,Y_test)) \n",
    "y_pred = grid.predict(X_test_std)\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the optimal Neural Network with 2 Layers on the whole Training Data\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=36, activation='sigmoid'))\n",
    "model.add(Dense(150, activation='sigmoid'))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(X_train_std, Y_train, validation_data=(X_test_std, Y_test), epochs=200, batch_size=20, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train_std, Y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test_std, Y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there was no improvement in accuracy of the two layered neural net compared to the single layered one we don't try to fit a three layered model. Also according to the universal approximation theorem a neural network with a single hidden layer containing a finite number of neurons can approximate any continuous functions and therefore a single layered network should be enough to fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagging Ratings\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "X_train_lag = pd.read_csv(\"X_train_lag.csv\")\n",
    "X_train_lag = X_train_lag.iloc[:, 1:]\n",
    "X_test_lag = pd.read_csv(\"X_test_lag.csv\")\n",
    "X_test_lag = X_test_lag.iloc[:, 1:]\n",
    "Y_train_lag = pd.read_csv(\"y_train_lag.csv\")\n",
    "Y_test_lag = pd.read_csv(\"y_test_lag.csv\")\n",
    "Y_train_lag = Y_train_lag['Lagging-Rating_1M'].astype('category') #factorize trainset\n",
    "Y_test_lag = Y_test_lag['Lagging-Rating_1M'].astype('category')   #factorize testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "param_grid = {'logreg__penalty': ['elasticnet'], #elastic nets combines l1&l2\n",
    "              'logreg__C':[7],\n",
    "              'logreg__l1_ratio':[0]} #if 0, or 1 then l2 or l1 would be best. If between then the combination of both\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "grid = functions.LogReg(X_train_lag,Y_train_lag,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(grid.best_score_))\n",
    "print('Test score:       {:.4f}'.format(grid.score(X_test_lag, Y_test_lag)))\n",
    "print('Best parameters: {}'.format(grid.best_params_))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes\n",
    "y_pred = grid.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######Polynomial Kernel Function#######\n",
    "param_grid = {'svm_poly__C': [1000], \n",
    "              'svm_poly__degree': [4],\n",
    "              'svm_poly__gamma': [0.05],\n",
    "              'svm_poly__coef0':[0.6]}\n",
    "    \n",
    "print(datetime.datetime.now()) #computation time\n",
    "poly = functions.SVM_poly(X_train_lag,Y_train_lag,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(poly.best_score_))\n",
    "print('Test score:       {:.4f}'.format(poly.score(X_test_lag, Y_test_lag)))\n",
    "print('Best parameters: {}'.format(poly.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = poly.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf)#######\n",
    "param_grid = {'svm_rbf__C': [150], \n",
    "              'svm_rbf__gamma': [0.3]} \n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf = functions.SVM_rbf(X_train_lag,Y_train_lag,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(rbf.best_score_))\n",
    "print('Test score:       {:.4f}'.format(rbf.score(X_test_lag, Y_test_lag)))\n",
    "print('Best parameters: {}'.format(rbf.best_params_))\n",
    "print(datetime.datetime.now()) \n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print('Radial Basis Function Kernel yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n",
    "\n",
    "\n",
    "#######Radial Basis Kernel Function(rbf) with Balanced class weights#######\n",
    "param_grid = {'svm_rbf__C': [200], \n",
    "              'svm_rbf__gamma': [0.3]} \n",
    "print(datetime.datetime.now()) #computation time\n",
    "rbf_bal = functions.SVM_rbf_bal(X_train_lag,Y_train_lag,param_grid)\n",
    "print('Best CV accuracy: {:.4f}'.format(rbf_bal.best_score_))\n",
    "print('Test score:       {:.4f}'.format(rbf_bal.score(X_test_lag, Y_test_lag)))\n",
    "print('Best parameters: {}'.format(rbf_bal.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = rbf_bal.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print('Radial Basis Function Kernel with Balanced class weights yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define arrays of values to be tested for paramgrid inside GridSearchCV function\n",
    "\n",
    "#number of trees in the forest\n",
    "n_estimators = np.array([70])\n",
    "\n",
    "# Max depth\n",
    "maxDepth = np.array([25])\n",
    "\n",
    "# Minimum number of samples required to split any internal node \n",
    "minSamplesNode = np.array([2])\n",
    "\n",
    "# The minimum number of samples required to be at a leaf/terminal node\n",
    "minSamplesLeaf = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RandomForest Classifier\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "forest = functions.random_forest(X_train_lag,Y_train_lag, n_estimators, maxDepth, minSamplesNode, minSamplesLeaf)\n",
    "print('Best CV accuracy: {:.4f}'.format(forest.best_score_))\n",
    "print('Test score:       {:.4f}'.format(forest.score(X_test_lag,Y_test_lag)))\n",
    "print('Best parameters: {}'.format(forest.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = forest.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define arrays of values to be tested for paramgrid inside GridSearchCV function\n",
    "\n",
    "#The ith element represents the number of neurons in the ith hidden layer.\n",
    "hidden_lay = np.array([400])\n",
    "\n",
    "# Max number of iterations\n",
    "max_Iter = np.array([575])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run MLP Classifier\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "mlp = functions.neural(X_train_lag,Y_train_lag, hidden_lay, max_Iter)\n",
    "print('Best CV accuracy: {:.4f}'.format(mlp.best_score_))\n",
    "print('Test score:       {:.4f}'.format(mlp.score(X_test_lag,Y_test_lag)))\n",
    "print('Best parameters: {}'.format(mlp.best_params_))\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Predict classes\n",
    "y_pred = mlp.predict(X_test_lag)\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print('Polynomial Kernel Function yields the following confusion matrix:')\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train_lag)\n",
    "X_train_lag_std = scaler.transform(X_train_lag) #standardizing using StandardScaler\n",
    "X_test_lag_std = scaler.transform(X_test_lag) #standardizing using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1Layer Neural Net(According to the Universal Approximation Theorem theoretically a 1 Layer Neural Net can approximate any other Neural Network):\n",
    "#Final model after Hyperparameter optimization. We get a 96% accuracy for the testset\n",
    "dropout_rate = [0.0]\n",
    "optimizer = ['Nadam']\n",
    "activ = ['sigmoid']\n",
    "neurons = [350]\n",
    "epochs=[200]\n",
    "batch_size=[10]\n",
    "print(datetime.datetime.now())\n",
    "# model fitting\n",
    "\n",
    "param_grid = dict(dropout_rate=dropout_rate, optimizer=optimizer,activ=activ,neurons=neurons,epochs=epochs,batch_size=batch_size)\n",
    "grid = functions.NeuralNet1layer(X_train_lag_std,Y_train_lag,param_grid)\n",
    "#results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Test score:', grid.score(X_test_lag_std,Y_test_lag)) \n",
    "y_pred = grid.predict(X_test_lag_std)\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the optimal Neural Network with 1 Layer on the whole Training Data\n",
    "model = Sequential()\n",
    "model.add(Dense(350, input_dim=36, activation='sigmoid'))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(X_train_lag_std, Y_train_lag, validation_data=(X_test_lag_std, Y_test_lag), epochs=200, batch_size=10, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train_lag_std, Y_train_lag, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test_lag_std, Y_test_lag, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2Layer Neural Net:\n",
    "activ1 = ['sigmoid']\n",
    "activ2 = ['sigmoid']\n",
    "neurons1 = [400]\n",
    "neurons2 = [150]\n",
    "dropout_rate1 = [0.0]\n",
    "dropout_rate2 = [0.0]\n",
    "print(datetime.datetime.now())\n",
    "# model fitting\n",
    "\n",
    "param_grid = dict(activ1=activ1,activ2=activ2,neurons1=neurons1,neurons2=neurons2,dropout_rate1=dropout_rate1,dropout_rate2=dropout_rate2)\n",
    "grid = functions.NeuralNet2layer(X_train_lag_std,Y_train_lag,param_grid)\n",
    "#results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Test score:', grid.score(X_test_lag_std,Y_test_lag)) \n",
    "y_pred = grid.predict(X_test_lag_std)\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted': y_pred,\n",
    "                      'True': Y_test_lag})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted')) \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the optimal Neural Network with 2 Layers on the whole Training Data\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=36, activation='sigmoid'))\n",
    "model.add(Dense(150, activation='sigmoid'))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(X_train_std, Y_train, validation_data=(X_test_lag_std, Y_test_lag), epochs=200, batch_size=20, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train_lag_std, Y_train_lag, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test_lag_std, Y_test_lag, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "plt.style.use('default')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use('default')\n",
    "# Compute confusion matrix\n",
    "confm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confm, classes=['A', 'A+', 'AA-', 'AAA', 'BBB', 'BBB-', 'AA', 'A-', 'BBB+', 'AA+', 'CCC+', \n",
    "                                     'B-', 'B+', 'BB-', 'BB+', 'BB', 'B'],\n",
    "                      title='Confusion matrix, without normalization');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.bincount(y_pred))\n",
    "print(pd.factorize(df[\"splticrm\"])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', metrics.accuracy_score(Y_test, y_pred))\n",
    "print('Error Rate:', 1-metrics.accuracy_score(Y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "confm = pd.DataFrame({'Predicted': y_pred,'True': Y_test})\n",
    "print(confm.groupby(['True','Predicted'], sort=True).size().unstack('Predicted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the precision, recall, f1-score, accuracy for each class. Support stands for number of samples for each class in Y_test. Macro avg is unweighted average between all rating groups. Weighted avg is weighted average between all rating groups. (weighted by support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite not being very visually appealing, the following package shows all possible Performance Metrics one might want to use in the multiclass classification case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycm import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "Y_test_cm = np.array(Y_test)\n",
    "cm = ConfusionMatrix(actual_vector=Y_test_cm, predict_vector=y_pred) # Create CM From Data\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = confm.sum(axis=0) - np.diag(confm)  \n",
    "FN = confm.sum(axis=1) - np.diag(confm)\n",
    "TP = np.diag(confm)\n",
    "TN = confm.sum() - (FP + FN + TP)\n",
    "\n",
    "# Metrics\n",
    "Score = (TP+TN)/(TP+FP+FN+TN)\n",
    "Errorrate = (FP+FN)/(TP+FP+FN+TN)\n",
    "Specifity = TN/(TN+FP)\n",
    "Sensivity = TP/(TP+FN)\n",
    "Precision = TP/(TP+FP)\n",
    "F1 = ((1+1**2)*TP)/((1+1**2)*TP+FN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
